{
  "llm_config": {
    "model": "mistral",
    "base_url": "http://localhost:11434/v1",
    "api_type": "ollama",
    "temperature": 0.7
  }
}
